dry_run: true

defaults:
  # This makes the logging module logs colorful
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

logging:
  steps_per_log: 10
  # stats tracker logs configurable training stats per iteration to terminal (e.g. rays/s, PSNR)
  enable_stats: True
  stats_tracker:
    max_history: 10
    stats_to_track: [ITER_LOAD_TIME, ITER_TRAIN_TIME] # see mattport/utils/stats_tracker.py for options
  # writer logs losses and images per iteration
  writer: 
    type: TensorboardWriter
    save_dir: './'
  # profiler logs run times of functions and prints at end of training
  enable_profiler: True

hydra:
  run:
    # Configure output dir of each experiment programmatically from the arguments
    # Example "outputs/mnist/classifier/baseline/2021-03-10-141516"
    dir: outputs/${experiment_name}/${method_name}/${now:%Y-%m-%d_%H%M%S}

experiment_name: blender_lego
method_name: vanilla_nerf

data:
  # dataset options
  dataset:
    data_directory: data/blender/lego_test
    dataset_type: blender
    downscale_factor: 4

  # dataloader options
  dataloader:
    num_workers: 4
    shuffle: true
    num_images_to_sample_from: 1
    num_rays_per_batch: 8
    num_times_to_repeat_images: 40

# machine options
machine_config:
  num_gpus: 0
  num_machines: 1
  machine_rank: 0
  dist_url: auto

# graph options
graph:
  model_dir: /tmp/mattport_models/
  max_num_iterations: 10
  steps_per_save: null
  steps_per_test: 100

  # pretrained loading options
  resume_train:
    load_dir: # pretrained model directory
    load_step: # pretrained model's step number

  network:
    # NOTE(ethan): currently setting to nothing b/c we're still designing the config format...
    _target_: mattport.nerf.graph.vanilla_nerf.NeRFGraph
    near_plane: 1.0
    far_plane: 6.0
    num_coarse_samples: 4
    num_importance_samples: 4
  # optimizer options
  param_groups:
    fields:
      optimizer:
        type: Adam
        lr: 1e-4
      scheduler:
        type: StepLR
        step_size: 30 #  Period of learning rate decay.
        gamma: 0.1 # Multiplicative factor of learning rate decay. Default: 0.1.
    cameras:
      optimizer:
        type: SGD
        lr: 1e-4
      scheduler:
        type: ExponentialLR
        gamma: 0.1